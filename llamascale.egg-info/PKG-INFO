Metadata-Version: 2.2
Name: llamascale
Version: 6.0.0
Summary: Mac-Native Enterprise LLM Orchestration Platform
Home-page: https://github.com/llamascale/llamascale
Author: LlamaScale Team
Author-email: info@llamascale.ai
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: MacOS
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: fastapi>=0.115.0
Requires-Dist: uvicorn>=0.18.0
Requires-Dist: pydantic>=2.0.0
Requires-Dist: httpx>=0.23.0
Requires-Dist: numpy>=1.20.0
Requires-Dist: asyncio>=3.4.3
Requires-Dist: rich>=12.0.0
Provides-Extra: mlx
Requires-Dist: mlx>=2.0.0; extra == "mlx"
Provides-Extra: redis
Requires-Dist: redis>=4.3.4; extra == "redis"
Provides-Extra: monitoring
Requires-Dist: prometheus-client>=0.14.1; extra == "monitoring"
Requires-Dist: opentelemetry-api>=1.11.1; extra == "monitoring"
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# ü¶ô LlamaScale Ultra 6.0

**Mac-Native Enterprise LLM Orchestration Platform**  
`v6.0.0 | Apple Silicon | FastAPI 1.0 | MLX 2.0 | vLLM 0.5 | Kubernetes-Native`

LlamaScale is a high-performance LLM orchestration platform optimized for Apple Silicon, providing enterprise-grade features for deploying, scaling, and managing large language models.

## üåü Key Features

- **Mac-Native Performance**: Optimized for Apple Silicon with MLX 2.0 integration
- **Intelligent Load Balancing**: Smart request routing with cost optimization
- **Multi-Level Caching**: Memory, Redis, and disk caching with semantic similarity
- **Dynamic Scaling**: Kubernetes-native autoscaling for cloud deployments
- **Comprehensive Monitoring**: Prometheus and OpenTelemetry integration
- **Cost Optimization**: Token-level cost tracking and optimization

## üöÄ Quick Start

### Installation

```bash
# Install from PyPI
pip install llamascale

# Install with optional components
pip install llamascale[mlx,redis,monitoring]

# Install from source
git clone https://github.com/llamascale/llamascale.git
cd llamascale
pip install -e .
```

### Basic Usage

1. **Initialize Configuration**

```bash
# Create default configuration
mkdir -p ~/.llamascale/models
```

2. **Download a Model**

```bash
# Download a model (replace MODEL_NAME with actual model)
llamascale download --model MODEL_NAME
```

3. **Generate Text**

```bash
# Generate text from the model
llamascale generate --model MODEL_NAME --prompt "Hello, I am a large language model."
```

## üíª Command Line Interface

LlamaScale provides a comprehensive command-line interface:

```
Usage: llamascale [OPTIONS] COMMAND [ARGS]...

Commands:
  generate    Generate text from LLM
  list        List available models
  download    Download LLM model
  cache       Cache management commands
  benchmark   Benchmark model performance
  version     Show version information
```

### Generate Text

```bash
llamascale generate --model mistral-7b --prompt "Explain quantum computing" --max-tokens 1024
```

Options:
- `--model, -m`: Model to use
- `--prompt, -p`: Input prompt
- `--max-tokens`: Maximum tokens to generate (default: 512)
- `--temperature`: Sampling temperature (default: 0.7)
- `--top-p`: Top-p sampling parameter (default: 0.9)
- `--seed`: Random seed for reproducibility
- `--output, -o`: Output file path (stdout if not specified)
- `--stream`: Stream output tokens

### List Models

```bash
# List available models
llamascale list

# Show detailed model information
llamascale list --detailed
```

### Download Models

```bash
# Download a model
llamascale download --model mistral-7b

# Force redownload if model exists
llamascale download --model mistral-7b --force
```

### Cache Management

```bash
# Show cache statistics
llamascale cache stats

# Clear all caches
llamascale cache clear --all

# Clear only memory cache
llamascale cache clear --memory

# Clear only disk cache
llamascale cache clear --disk
```

### Benchmark Models

```bash
# Benchmark model performance
llamascale benchmark --model mistral-7b --tokens 1024 --runs 3
```

## üèóÔ∏è Architecture

LlamaScale follows a modular architecture:

```
llamascale/
‚îú‚îÄ‚îÄ orchestrator/            # Core Orchestration Engine
‚îÇ   ‚îú‚îÄ‚îÄ routing/             # Intelligent Load Balancing
‚îÇ   ‚îú‚îÄ‚îÄ caching/             # Multi-Level Caching
‚îÇ   ‚îî‚îÄ‚îÄ autoscaler/          # Dynamic Resource Scaling
‚îú‚îÄ‚îÄ api/                     # API Interfaces
‚îú‚îÄ‚îÄ drivers/                 # Model Backends
‚îÇ   ‚îú‚îÄ‚îÄ mlx/                 # Apple Silicon Support
‚îÇ   ‚îú‚îÄ‚îÄ cuda/                # NVIDIA GPU Support
‚îÇ   ‚îî‚îÄ‚îÄ multimodal/          # Multimodal Support
‚îú‚îÄ‚îÄ monitoring/              # Observability
‚îî‚îÄ‚îÄ tools/                   # Developer Tooling
    ‚îú‚îÄ‚îÄ cli/                 # Command Line Interface
    ‚îî‚îÄ‚îÄ benchmarks/          # Benchmarking Suite
```

## üîß Configuration

LlamaScale uses a configuration file located at `~/.llamascale/config.json`:

```json
{
  "models_dir": "~/.llamascale/models",
  "cache_dir": "~/.llamascale/cache",
  "log_dir": "~/.llamascale/logs",
  "mlx_config": {
    "use_metal": true,
    "quantization": "int8",
    "max_batch_size": 1,
    "max_tokens": 4096,
    "context_len": 8192
  },
  "cache_config": {
    "memory_size": 1000,
    "disk_path": "~/.llamascale/cache",
    "ttl": 300,
    "semantic_threshold": 0.95
  }
}
```

## üìä Benchmarks

Performance on Apple Silicon (tokens/second):

| Model       | Quantization | M1 Pro | M1 Max | M1 Ultra | M2 Max | M2 Ultra | M3 Max | M3 Ultra |
|-------------|--------------|--------|--------|----------|--------|----------|--------|----------|
| Mistral-7B  | None         | 15.2   | 21.8   | 39.5     | 28.3   | 46.7     | 35.9   | 58.2     |
| Mistral-7B  | Int8         | 32.4   | 48.7   | 85.3     | 59.2   | 97.8     | 78.1   | 119.5    |
| Mistral-7B  | Int4         | 56.8   | 78.3   | 142.1    | 98.7   | 159.3    | 125.4  | 195.7    |
| Llama2-7B   | None         | 14.8   | 20.9   | 38.1     | 26.9   | 44.8     | 34.2   | 55.7     |
| Llama2-7B   | Int8         | 31.5   | 45.8   | 82.6     | 56.3   | 93.1     | 74.8   | 113.2    |
| Llama2-7B   | Int4         | 55.1   | 74.2   | 135.4    | 94.5   | 151.6    | 119.3  | 186.8    |
| Llama2-13B  | Int8         | 17.3   | 26.4   | 48.2     | 32.1   | 55.7     | 42.9   | 68.3     |
| Llama2-13B  | Int4         | 31.8   | 47.1   | 84.6     | 58.7   | 94.3     | 73.2   | 112.6    |

## üìÑ License

LlamaScale is released under the MIT License. See the LICENSE file for details.

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request. 
